※정적 page crawling
: beautifulSoup4 이용
페이지가 바뀌면 한번 더 requests를 해서 해당 페이지에 대한 데이터를 받아야한다. (스크롤도 동일)
페이지를 실행하지 않아도 데이터를 가져온다. (그래서 정보 처리 빠름)
코드로 처리하는 거라 오류를 발견하면 알려준다.

정적 page 예시 : 정부 페이지, 기업 페이지 
모두에게 동일한 정보를 제공하는 페이지들

※동적 page crawling
: selenium 이용
알아서 스크롤을 해서 동적으로 데이터를 가져옴. (정보 처리 느림)
페이지를 실행해야지 데이터를 가져올 수 있다.
모바일 형태로도 접속할 수 있다...!
어느정도 사람의 개입이 필요하다. (아무리 자동화라고 해도 사람이 지켜보면서 오류 확인해야함.)

동적 page 예시 : 유튜브, 넷플릭스, 쇼핑몰(지그재그, 에이블리, 무신사)

※페이지에서 원하는 정보 가져오기 (정적 크롤링)
(중간 중간 print 문은 받고 있는 정보가 무엇인지 확인하기 위해 실행한 코드들로 주석처리 하였다.)

1. 원하는 페이지의 url 입력 (변수로 저장)
2. get으로 서버에게 페이지의 자원(resource) 요청 
(requests 함수는 처음에 사용되고 이후에 잘 사용되지 않음.
정적 page에서는 코드가 변하지 않기 때문에)
3. html 문자로 받아오기 
(get 방식을 통해서 가져온 많은 데이터 중 
우리가 필요한건 텍스트 형태의 데이터이며 
이것을 html이라는 변수에 저장. 
이때는 컴퓨터가 이해하지 못하는 문서)
4. 받은 html 파일을 perser.(파싱) 
(BeautifulSoup 함수에는 2가지 파라미터 필요.
<html과 html.parser> 넣으면 파서 진행.
이렇게 해야 컴퓨터가 이해하는 문서(트리구조)로 변경)
5. 원하는 정보의 id를 찾아서 query 변수로 넣어줌.
(select_one은 class, id, 태그 찾아준다.)